{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8zsUp8wjkQ5"
      },
      "source": [
        "# **Face Swaps with Dlib**\n",
        "\n",
        "####**In this lesson we'll learn:**\n",
        "1. Perform Face Swapping using Dlib\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQzvD9O0jcS7"
      },
      "source": [
        "# Our Setup, Import Libaries, Create our Imshow Function and Download our Images\n",
        "import cv2\n",
        "import dlib\n",
        "import sys\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Define our imshow function \n",
        "def imshow(title = \"Image\", image = None, size = 10):\n",
        "    w, h = image.shape[0], image.shape[1]\n",
        "    aspect_ratio = w/h\n",
        "    plt.figure(figsize=(size * aspect_ratio,size))\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and unzip our images and Facial landmark model\n",
        "!wget https://moderncomputervision.s3.eu-west-2.amazonaws.com/images.zip\n",
        "!wget https://moderncomputervision.s3.eu-west-2.amazonaws.com/shape_predictor_68_face_landmarks.zip\n",
        "!unzip -qq images.zip\n",
        "!unzip -qq shape_predictor_68_face_landmarks.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40F_ygllUCdw",
        "outputId": "89d8e554-c6b6-4b50-c00a-029955b5f0f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-01 07:56:46--  https://moderncomputervision.s3.eu-west-2.amazonaws.com/images.zip\n",
            "Resolving moderncomputervision.s3.eu-west-2.amazonaws.com (moderncomputervision.s3.eu-west-2.amazonaws.com)... 52.95.142.34\n",
            "Connecting to moderncomputervision.s3.eu-west-2.amazonaws.com (moderncomputervision.s3.eu-west-2.amazonaws.com)|52.95.142.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29586716 (28M) [application/zip]\n",
            "Saving to: ‘images.zip’\n",
            "\n",
            "images.zip          100%[===================>]  28.22M  18.5MB/s    in 1.5s    \n",
            "\n",
            "2023-05-01 07:56:48 (18.5 MB/s) - ‘images.zip’ saved [29586716/29586716]\n",
            "\n",
            "--2023-05-01 07:56:48--  https://moderncomputervision.s3.eu-west-2.amazonaws.com/shape_predictor_68_face_landmarks.zip\n",
            "Resolving moderncomputervision.s3.eu-west-2.amazonaws.com (moderncomputervision.s3.eu-west-2.amazonaws.com)... 52.95.142.34\n",
            "Connecting to moderncomputervision.s3.eu-west-2.amazonaws.com (moderncomputervision.s3.eu-west-2.amazonaws.com)|52.95.142.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72326300 (69M) [application/zip]\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.zip’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  68.98M  23.5MB/s    in 2.9s    \n",
            "\n",
            "2023-05-01 07:56:52 (23.5 MB/s) - ‘shape_predictor_68_face_landmarks.zip’ saved [72326300/72326300]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6ORUGNgjuQ7"
      },
      "source": [
        "## **Performing Face Swaps**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Global variables and path to detector\n",
        "import sys\n",
        "\n",
        "PREDICTOR_PATH = \"shape_predictor_68_face_landmarks.dat\"\n",
        "SCALE_FACTOR = 1 \n",
        "FEATHER_AMOUNT = 11\n",
        "\n",
        "FACE_POINTS = list(range(17, 68))\n",
        "MOUTH_POINTS = list(range(48, 61))\n",
        "RIGHT_BROW_POINTS = list(range(17, 22))\n",
        "LEFT_BROW_POINTS = list(range(22, 27))\n",
        "RIGHT_EYE_POINTS = list(range(36, 42))\n",
        "LEFT_EYE_POINTS = list(range(42, 48))\n",
        "NOSE_POINTS = list(range(27, 35))\n",
        "JAW_POINTS = list(range(0, 17))"
      ],
      "metadata": {
        "id": "HjaJP7gAUGWR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Points used to line up the images.\n",
        "ALIGN_POINTS = (LEFT_BROW_POINTS + RIGHT_EYE_POINTS + LEFT_EYE_POINTS +\n",
        "                               RIGHT_BROW_POINTS + NOSE_POINTS + MOUTH_POINTS)\n",
        "\n",
        "# Points from the second image to overlay on the first. The convex hull of each\n",
        "# element will be overlaid.\n",
        "OVERLAY_POINTS = [\n",
        "    LEFT_EYE_POINTS + RIGHT_EYE_POINTS + LEFT_BROW_POINTS + RIGHT_BROW_POINTS,\n",
        "    NOSE_POINTS + MOUTH_POINTS,\n",
        "]\n",
        "\n",
        "# Amount of blur to use during colour correction, as a fraction of the\n",
        "# pupillary distance.\n",
        "COLOUR_CORRECT_BLUR_FRAC = 0.6\n",
        "\n",
        "# Initialize Detector\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(PREDICTOR_PATH)"
      ],
      "metadata": {
        "id": "HcqR1J2QUTMf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect Facial Landmark\n",
        "class TooManyFaces(Exception):\n",
        "  pass\n",
        "\n",
        "class noFaces(Exception):\n",
        "  pass\n",
        "\n",
        "def get_landmarks(img):\n",
        "  # Return coordinates(x,y)\n",
        "  ret = detector(img, 1)\n",
        "\n",
        "  if len(ret) > 1:\n",
        "    raise TooManyFaces\n",
        "  if len(ret) == 0:\n",
        "    raise NoFaces\n",
        "\n",
        "  matrices = [[p.x, p.y] for p in predictor(img, ret[0]).parts()]\n",
        "  \n",
        "  return np.matrix(matrices)"
      ],
      "metadata": {
        "id": "QxP4f3sCUamT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def annotate_landmarks(img, landmarks):\n",
        "  # Overlay the points from landmark to original image\n",
        "\n",
        "  img = img.copy()\n",
        "\n",
        "  # Iterate through the landmarks and draw landmarks in original image\n",
        "  for idx, point in enumerate(landmarks):\n",
        "    pos = (point[0, 0], point[0, 1])\n",
        "    cv2.putText(img, str(idx), pos, fontFace=cv2.FONT_HERSHEY_SCRIPT_COMPLEX, fontScale=0.3, color=(0,255, 0))\n",
        "    cv2.circle(img, pos, 3, color=(255, 255,0))\n",
        "\n",
        "  return img"
      ],
      "metadata": {
        "id": "hb9OcmYrVZxf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drawconvex used to detect outer points for the landmarks\n",
        "# More info here: https://docs.opencv.org/3.4/d7/d1d/tutorial_hull.html\n",
        "def draw_convex_hull(img, points, color):\n",
        "  points = cv2.convexHull(points)\n",
        "  cv2.fillConvexPoly(img, points, color=color)"
      ],
      "metadata": {
        "id": "XqyrfuANV0vV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}